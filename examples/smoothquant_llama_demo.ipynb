{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmoothQuant on OPT-13B\n",
    "\n",
    "### Guangxuan Xiao\\*, Ji Lin\\*, Mickael Seznec, Julien Demouth, Song Han\n",
    "\n",
    "In this notebook, we use OPT-13B model to demonstrate SmoothQuant can use 8-bit for both weights and activations to achieve the same accuracy as FP16 models. Unlike previous method [[Dettmers *et al.*, 2022]](https://arxiv.org/abs/2208.07339), SmoothQuant enables fully INT8 GEMMs for linear layers and does not require high precision numbers to represent outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates SmoothQuant on OPT-13B in consideration of the user's resouce constraints. We have tested SmoothQuant on up to 176 billion parameter models (OPT-175B, BLOOM-176B, GLM-130B). You can also adjust the model name to validate SmoothQuant on other models. `../act_scales/` provides the activation channel scales for OPT and BLOOM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- smoothquant\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.opt.modeling_opt import (\n",
    "    OPTAttention, OPTDecoderLayer, OPTForCausalLM)\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaAttention, LlamaDecoderLayer, LlamaRMSNorm) #, LlamaForCausalLM)\n",
    "\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.fake_quant import W8A8Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we simulate the 8-bit dynamic per-tensor weight and activation quantization with FP16, i.e., fake quantization. We have implemented the real 8-bit quantization with INT8 CUTLASS GEMM kernels for both PyTorch and FasterTransformer. Please stay tuned for the release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, weight_quant='per_tensor', act_quant='per_tensor', quantize_bmm_input=True):\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, LlamaDecoderLayer):  # OPTDecoderLayer\n",
    "            # m.fc1 = W8A8Linear.from_float(m.fc1, weight_quant=weight_quant, act_quant=act_quant)\n",
    "            # m.fc2 = W8A8Linear.from_float(m.fc2, weight_quant=weight_quant, act_quant=act_quant)\n",
    "            \n",
    "            m.self_attn.q_proj = W8A8Linear.from_float(\n",
    "                m.self_attn.q_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.self_attn.k_proj = W8A8Linear.from_float(\n",
    "                m.self_attn.k_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.self_attn.v_proj = W8A8Linear.from_float(\n",
    "                m.self_attn.v_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.self_attn.o_proj = W8A8Linear.from_float(\n",
    "                m.self_attn.o_proj, weight_quant=weight_quant, act_quant=act_quant)\n",
    "            \n",
    "            m.mlp.gate_proj = W8A8Linear.from_float(\n",
    "                m.mlp.gate_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.mlp.up_proj = W8A8Linear.from_float(\n",
    "                m.mlp.up_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.mlp.down_proj = W8A8Linear.from_float(\n",
    "                m.mlp.down_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "        elif isinstance(m, LlamaAttention):  # OPTAttention\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            # print(f'!!! {m}')\n",
    "            continue\n",
    "            \n",
    "            print(m.q_proj)\n",
    "            m.q_proj = W8A8Linear.from_float(\n",
    "                m.q_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.k_proj = W8A8Linear.from_float(\n",
    "                m.k_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.v_proj = W8A8Linear.from_float(\n",
    "                m.v_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.out_proj = W8A8Linear.from_float(m.out_proj, weight_quant=weight_quant, act_quant=act_quant)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 1000 examples in the validation set of the Lambada dataset) to evaluate the model. You can replace it with your own dataset. The conclusion should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**In this demo, we have simplified the evaluation by using the first 1,000 samples from the LAMBADA dataset's validation set. We employ the \"Last Token Prediction Accuracy\" as our evaluation metric. This approximate evaluation is intended for demonstration purposes, providing simple but meaningful comparisons of relative performance between methods. For a more strict assessment, we recommend using the [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness) to obtain the \"Last Word Prediction Accuracy\" for the LAMBADA dataset, which is the reported metric in our paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        # tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            example = self.tokenizer(examples['text'], )\n",
    "            return example\n",
    "\n",
    "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "        self.dataset.set_format(type='torch', columns=['input_ids'])\n",
    "        \n",
    "#         for batch in self.dataset:\n",
    "#             print(batch['input_ids'].shape)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        # The task is to predict the last word of the input.\n",
    "        total, hit = 0, 0\n",
    "        for i, batch in enumerate(self.dataset):\n",
    "            input_ids = batch['input_ids'].to(self.device).unsqueeze(0)\n",
    "            \n",
    "            if input_ids.shape[1] < 5:\n",
    "                # print(f'!!! Skip input of shape {input_ids.shape} !!!')\n",
    "                continue\n",
    "            \n",
    "            label = input_ids[:, -1]\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "            \n",
    "            # print(input_ids.shape)\n",
    "            # print(outputs)\n",
    "            # print(outputs.logits.shape)\n",
    "            # assert False\n",
    "            \n",
    "            last_token_logits = outputs.logits[:, -1, :]  # TODO: ???\n",
    "            # -1 or -2 ???\n",
    "            \n",
    "            pred = last_token_logits.argmax(dim=-1)\n",
    "            \n",
    "            #if i < 10:\n",
    "            #    print('From evaluate:')\n",
    "            #    print(input_ids)\n",
    "            #    print(pred)\n",
    "            \n",
    "            total += label.size(0)\n",
    "            hit += (pred == label).sum().item()\n",
    "        acc = hit / total\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-7b\r\n"
     ]
    }
   ],
   "source": [
    "! ls ~/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alekseev_v/projects_node4/smoothquant/examples\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/home/alekseev_v/models/llama-7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████| 1000/1000 [00:00<00:00, 2343.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('facebook/opt-13b')\n",
    "#dataset = load_dataset('lambada', split='validation[:1000]')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation[:1000]')\n",
    "\n",
    "# evaluator = Evaluator(dataset, tokenizer, 'cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the performance of the original FP16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████| 33/33 [00:30<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# OPTForCausalLM\n",
    "\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, torch_dtype=torch.float16, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████| 1000/1000 [00:00<00:00, 2973.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(dataset, tokenizer, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "Original model (fp16) accuracy: 0.968944099378882\n"
     ]
    }
   ],
   "source": [
    "acc_fp16 = evaluator.evaluate(model_fp16)\n",
    "print(f'Original model (fp16) accuracy: {acc_fp16}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then quantize the model to W8A8 and check the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "!!! LlamaAttention(\n",
      "  (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (1): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (2): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (3): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (27): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (29): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (30): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (31): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_w8a8 = quantize_model(model_fp16)\n",
    "print(model_w8a8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "!!! Skip input of shape torch.Size([1, 1]) !!!\n",
      "Naive W8A8 quantized model accuracy: 0.10093167701863354\n"
     ]
    }
   ],
   "source": [
    "acc_w8a8 = evaluator.evaluate(model_w8a8)\n",
    "print(f'Naive W8A8 quantized model accuracy: {acc_w8a8}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a significant accuracy drop. This is consistent with LLM.int8()'s finding: when the model size increases larger than 6.7B, systematic outliers will emerge in activations, which makes fully INT8 quantization impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "from transformers.models.bloom.modeling_bloom import BloomBlock\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_ln_fcs(ln, fcs, act_scales, alpha=0.5):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "        \n",
    "    assert isinstance(ln, LlamaRMSNorm) #nn.LayerNorm), type(ln)\n",
    "    \n",
    "    for fc in fcs:\n",
    "        assert isinstance(fc, nn.Linear)\n",
    "        assert ln.weight.numel() == fc.in_features == act_scales.numel()\n",
    "\n",
    "    device, dtype = fcs[0].weight.device, fcs[0].weight.dtype\n",
    "    act_scales = act_scales.to(device=device, dtype=dtype)\n",
    "    weight_scales = torch.cat([fc.weight.abs().max(\n",
    "        dim=0, keepdim=True)[0] for fc in fcs], dim=0)\n",
    "    weight_scales = weight_scales.max(dim=0)[0].clamp(min=1e-5)\n",
    "\n",
    "    scales = (act_scales.pow(alpha) / weight_scales.pow(1-alpha)\n",
    "              ).clamp(min=1e-5).to(device).to(dtype)\n",
    "\n",
    "    ln.weight.div_(scales)\n",
    "    # ln.bias.div_(scales)\n",
    "\n",
    "    for fc in fcs:\n",
    "        fc.weight.mul_(scales.view(1, -1))\n",
    "    \n",
    "    return scales\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_lm(model, scales, alpha=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LlamaDecoderLayer):  # OPTDecoderLayer\n",
    "            attn_ln = module.input_layernorm  # self_attn_layer_norm\n",
    "            qkv = [module.self_attn.q_proj,\n",
    "                   module.self_attn.k_proj,\n",
    "                   module.self_attn.v_proj,]\n",
    "                   # module.self_attn.o_proj]  # ???\n",
    "            qkv_input_scales = scales[name + '.self_attn.q_proj']\n",
    "            smooth_ln_fcs(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "\n",
    "            \n",
    "            ffn_ln = module.post_attention_layernorm  # final_layer_norm\n",
    "            #fc1 = module.mlp.gate_proj  # ??? dense_h_to_4h\n",
    "            fc1 = [\n",
    "                module.mlp.gate_proj, module.mlp.up_proj\n",
    "            ]\n",
    "            fc1_input_scales = scales[name + '.mlp.gate_proj']\n",
    "            \n",
    "            _scales = smooth_ln_fcs(ffn_ln, fc1, fc1_input_scales, alpha)\n",
    "            \n",
    "            # print(_scales)\n",
    "            \n",
    "        \"\"\"\n",
    "        elif isinstance(module, BloomBlock):\n",
    "            attn_ln = module.input_layernorm\n",
    "            qkv = module.self_attention.query_key_value\n",
    "            qkv_input_scales = scales[name + '.self_attention.query_key_value']\n",
    "            smooth_ln_fcs(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "\n",
    "            ffn_ln = module.post_attention_layernorm\n",
    "            fc1 = module.mlp.gate_proj  # ??? dense_h_to_4h\n",
    "            fc1_input_scales = scales[name + '.mlp.gate_proj']\n",
    "            smooth_ln_fcs(ffn_ln, fc1, fc1_input_scales, alpha)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's smooth the model, quantize it, and check the performance! In `../act_scales`, we provide the activation scales for OPT and BLOOM models. You can also use this notebook to test quantizing those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../act_scales/llama-7b.pt\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../act_scales/llama-7b.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████| 33/33 [00:42<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = OPTForCausalLM.from_pretrained(\n",
    "#     'facebook/opt-13b', torch_dtype=torch.float16, device_map='auto')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, torch_dtype=torch.float16, device_map='auto')\n",
    "\n",
    "act_scales = torch.load('../act_scales/llama-7b.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.layers.0.self_attn.q_proj': tensor([0.1954, 0.0732, 0.0819,  ..., 0.1273, 0.0908, 0.1018]),\n",
       " 'model.layers.0.self_attn.k_proj': tensor([0.1954, 0.0732, 0.0819,  ..., 0.1273, 0.0908, 0.1018]),\n",
       " 'model.layers.0.self_attn.v_proj': tensor([0.1954, 0.0732, 0.0819,  ..., 0.1273, 0.0908, 0.1018]),\n",
       " 'model.layers.0.self_attn.o_proj': tensor([0.0148, 0.0150, 0.0174,  ..., 0.1532, 0.0299, 0.0484]),\n",
       " 'model.layers.0.mlp.gate_proj': tensor([0.1642, 0.2338, 0.2385,  ..., 0.1931, 0.2471, 0.2076]),\n",
       " 'model.layers.0.mlp.up_proj': tensor([0.1642, 0.2338, 0.2385,  ..., 0.1931, 0.2471, 0.2076]),\n",
       " 'model.layers.0.mlp.down_proj': tensor([0.1388, 0.5039, 0.4951,  ..., 0.1766, 0.3137, 0.2393]),\n",
       " 'model.layers.1.self_attn.q_proj': tensor([0.4578, 0.2279, 0.2401,  ..., 0.1924, 0.2510, 0.2461]),\n",
       " 'model.layers.1.self_attn.k_proj': tensor([0.4578, 0.2279, 0.2401,  ..., 0.1924, 0.2510, 0.2461]),\n",
       " 'model.layers.1.self_attn.v_proj': tensor([0.4578, 0.2279, 0.2401,  ..., 0.1924, 0.2510, 0.2461]),\n",
       " 'model.layers.1.self_attn.o_proj': tensor([0.0980, 0.0196, 0.0203,  ..., 0.0326, 0.0372, 0.1238]),\n",
       " 'model.layers.1.mlp.gate_proj': tensor([0.3062, 0.3269, 0.3386,  ..., 0.3438, 0.3870, 0.3374]),\n",
       " 'model.layers.1.mlp.up_proj': tensor([0.3062, 0.3269, 0.3386,  ..., 0.3438, 0.3870, 0.3374]),\n",
       " 'model.layers.1.mlp.down_proj': tensor([0.6216, 0.2419, 0.7256,  ..., 0.3203, 0.4043, 0.5767]),\n",
       " 'model.layers.2.self_attn.q_proj': tensor([0.4673, 0.3901, 0.4453,  ..., 0.4268, 0.4546, 0.4609]),\n",
       " 'model.layers.2.self_attn.k_proj': tensor([0.4673, 0.3901, 0.4453,  ..., 0.4268, 0.4546, 0.4609]),\n",
       " 'model.layers.2.self_attn.v_proj': tensor([0.4673, 0.3901, 0.4453,  ..., 0.4268, 0.4546, 0.4609]),\n",
       " 'model.layers.2.self_attn.o_proj': tensor([0.2554, 0.2554, 0.2234,  ..., 0.2593, 0.2469, 0.2476]),\n",
       " 'model.layers.2.mlp.gate_proj': tensor([0.5029, 0.4746, 0.4490,  ..., 0.4419, 0.4795, 0.4458]),\n",
       " 'model.layers.2.mlp.up_proj': tensor([0.5029, 0.4746, 0.4490,  ..., 0.4419, 0.4795, 0.4458]),\n",
       " 'model.layers.2.mlp.down_proj': tensor([0.3748, 0.3542, 0.9399,  ..., 0.4663, 0.7051, 1.5879]),\n",
       " 'model.layers.3.self_attn.q_proj': tensor([0.7002, 0.7241, 0.7539,  ..., 0.7002, 0.8267, 0.6987]),\n",
       " 'model.layers.3.self_attn.k_proj': tensor([0.7002, 0.7241, 0.7539,  ..., 0.7002, 0.8267, 0.6987]),\n",
       " 'model.layers.3.self_attn.v_proj': tensor([0.7002, 0.7241, 0.7539,  ..., 0.7002, 0.8267, 0.6987]),\n",
       " 'model.layers.3.self_attn.o_proj': tensor([0.2042, 0.3789, 0.2715,  ..., 0.4341, 0.5737, 0.4863]),\n",
       " 'model.layers.3.mlp.gate_proj': tensor([0.5918, 0.5537, 0.5317,  ..., 0.5977, 0.5991, 0.5713]),\n",
       " 'model.layers.3.mlp.up_proj': tensor([0.5918, 0.5537, 0.5317,  ..., 0.5977, 0.5991, 0.5713]),\n",
       " 'model.layers.3.mlp.down_proj': tensor([0.6270, 0.5122, 2.5430,  ..., 0.4297, 0.8481, 1.0430]),\n",
       " 'model.layers.4.self_attn.q_proj': tensor([0.7822, 0.8418, 0.7520,  ..., 0.8696, 0.8247, 0.8735]),\n",
       " 'model.layers.4.self_attn.k_proj': tensor([0.7822, 0.8418, 0.7520,  ..., 0.8696, 0.8247, 0.8735]),\n",
       " 'model.layers.4.self_attn.v_proj': tensor([0.7822, 0.8418, 0.7520,  ..., 0.8696, 0.8247, 0.8735]),\n",
       " 'model.layers.4.self_attn.o_proj': tensor([0.5483, 0.3667, 0.4226,  ..., 0.6992, 0.4941, 0.4929]),\n",
       " 'model.layers.4.mlp.gate_proj': tensor([0.6348, 0.5957, 0.6177,  ..., 0.6577, 0.6211, 0.6187]),\n",
       " 'model.layers.4.mlp.up_proj': tensor([0.6348, 0.5957, 0.6177,  ..., 0.6577, 0.6211, 0.6187]),\n",
       " 'model.layers.4.mlp.down_proj': tensor([0.5757, 1.6348, 0.6377,  ..., 0.7896, 0.3728, 0.4482]),\n",
       " 'model.layers.5.self_attn.q_proj': tensor([1.0537, 1.0293, 1.0615,  ..., 1.0215, 0.9141, 1.0039]),\n",
       " 'model.layers.5.self_attn.k_proj': tensor([1.0537, 1.0293, 1.0615,  ..., 1.0215, 0.9141, 1.0039]),\n",
       " 'model.layers.5.self_attn.v_proj': tensor([1.0537, 1.0293, 1.0615,  ..., 1.0215, 0.9141, 1.0039]),\n",
       " 'model.layers.5.self_attn.o_proj': tensor([1.1016, 0.9619, 0.9634,  ..., 1.0498, 0.8467, 0.7417]),\n",
       " 'model.layers.5.mlp.gate_proj': tensor([0.6626, 0.6411, 0.7251,  ..., 0.7168, 0.8242, 0.7490]),\n",
       " 'model.layers.5.mlp.up_proj': tensor([0.6626, 0.6411, 0.7251,  ..., 0.7168, 0.8242, 0.7490]),\n",
       " 'model.layers.5.mlp.down_proj': tensor([1.9385, 0.5166, 0.4902,  ..., 1.4639, 1.0039, 0.4988]),\n",
       " 'model.layers.6.self_attn.q_proj': tensor([1.0742, 1.0322, 0.9907,  ..., 1.0254, 1.0342, 1.0449]),\n",
       " 'model.layers.6.self_attn.k_proj': tensor([1.0742, 1.0322, 0.9907,  ..., 1.0254, 1.0342, 1.0449]),\n",
       " 'model.layers.6.self_attn.v_proj': tensor([1.0742, 1.0322, 0.9907,  ..., 1.0254, 1.0342, 1.0449]),\n",
       " 'model.layers.6.self_attn.o_proj': tensor([0.6572, 0.7988, 0.7065,  ..., 0.8203, 0.6748, 0.7998]),\n",
       " 'model.layers.6.mlp.gate_proj': tensor([0.6982, 0.6978, 0.7480,  ..., 0.6992, 0.7876, 0.7383]),\n",
       " 'model.layers.6.mlp.up_proj': tensor([0.6982, 0.6978, 0.7480,  ..., 0.6992, 0.7876, 0.7383]),\n",
       " 'model.layers.6.mlp.down_proj': tensor([1.2930, 0.6484, 1.1289,  ..., 0.7563, 0.9146, 0.4741]),\n",
       " 'model.layers.7.self_attn.q_proj': tensor([1.0557, 0.9595, 0.9702,  ..., 1.3018, 0.9302, 1.3770]),\n",
       " 'model.layers.7.self_attn.k_proj': tensor([1.0557, 0.9595, 0.9702,  ..., 1.3018, 0.9302, 1.3770]),\n",
       " 'model.layers.7.self_attn.v_proj': tensor([1.0557, 0.9595, 0.9702,  ..., 1.3018, 0.9302, 1.3770]),\n",
       " 'model.layers.7.self_attn.o_proj': tensor([0.8804, 0.9590, 0.8818,  ..., 0.5649, 0.4551, 0.6890]),\n",
       " 'model.layers.7.mlp.gate_proj': tensor([0.7324, 0.7300, 0.7148,  ..., 0.8843, 0.7329, 0.8618]),\n",
       " 'model.layers.7.mlp.up_proj': tensor([0.7324, 0.7300, 0.7148,  ..., 0.8843, 0.7329, 0.8618]),\n",
       " 'model.layers.7.mlp.down_proj': tensor([1.0732, 0.4480, 0.5469,  ..., 0.9980, 0.9185, 0.8389]),\n",
       " 'model.layers.8.self_attn.q_proj': tensor([1.1924, 1.0732, 1.1738,  ..., 1.1953, 1.1025, 1.2441]),\n",
       " 'model.layers.8.self_attn.k_proj': tensor([1.1924, 1.0732, 1.1738,  ..., 1.1953, 1.1025, 1.2441]),\n",
       " 'model.layers.8.self_attn.v_proj': tensor([1.1924, 1.0732, 1.1738,  ..., 1.1953, 1.1025, 1.2441]),\n",
       " 'model.layers.8.self_attn.o_proj': tensor([0.7754, 1.1299, 1.6934,  ..., 0.7979, 0.9971, 1.1035]),\n",
       " 'model.layers.8.mlp.gate_proj': tensor([0.7861, 0.7524, 0.8086,  ..., 0.7949, 0.7749, 0.7754]),\n",
       " 'model.layers.8.mlp.up_proj': tensor([0.7861, 0.7524, 0.8086,  ..., 0.7949, 0.7749, 0.7754]),\n",
       " 'model.layers.8.mlp.down_proj': tensor([0.7339, 0.6152, 0.6787,  ..., 0.9355, 0.4561, 1.6699]),\n",
       " 'model.layers.9.self_attn.q_proj': tensor([1.1074, 1.2275, 1.1465,  ..., 1.4170, 1.1260, 1.3809]),\n",
       " 'model.layers.9.self_attn.k_proj': tensor([1.1074, 1.2275, 1.1465,  ..., 1.4170, 1.1260, 1.3809]),\n",
       " 'model.layers.9.self_attn.v_proj': tensor([1.1074, 1.2275, 1.1465,  ..., 1.4170, 1.1260, 1.3809]),\n",
       " 'model.layers.9.self_attn.o_proj': tensor([0.8467, 0.9395, 0.6216,  ..., 1.3232, 1.1318, 0.9888]),\n",
       " 'model.layers.9.mlp.gate_proj': tensor([0.8950, 0.8242, 0.7563,  ..., 0.8564, 0.7842, 0.8140]),\n",
       " 'model.layers.9.mlp.up_proj': tensor([0.8950, 0.8242, 0.7563,  ..., 0.8564, 0.7842, 0.8140]),\n",
       " 'model.layers.9.mlp.down_proj': tensor([1.3652, 0.5874, 0.5400,  ..., 0.6460, 0.6421, 0.7251]),\n",
       " 'model.layers.10.self_attn.q_proj': tensor([1.2969, 1.3184, 1.1426,  ..., 1.3506, 1.3047, 1.3789]),\n",
       " 'model.layers.10.self_attn.k_proj': tensor([1.2969, 1.3184, 1.1426,  ..., 1.3506, 1.3047, 1.3789]),\n",
       " 'model.layers.10.self_attn.v_proj': tensor([1.2969, 1.3184, 1.1426,  ..., 1.3506, 1.3047, 1.3789]),\n",
       " 'model.layers.10.self_attn.o_proj': tensor([0.9663, 1.1660, 1.1328,  ..., 1.0908, 0.5996, 0.6733]),\n",
       " 'model.layers.10.mlp.gate_proj': tensor([0.9072, 0.8960, 0.7925,  ..., 0.8301, 0.8911, 0.8555]),\n",
       " 'model.layers.10.mlp.up_proj': tensor([0.9072, 0.8960, 0.7925,  ..., 0.8301, 0.8911, 0.8555]),\n",
       " 'model.layers.10.mlp.down_proj': tensor([1.2588, 2.2754, 3.1152,  ..., 1.1641, 0.6143, 1.4561]),\n",
       " 'model.layers.11.self_attn.q_proj': tensor([1.3721, 1.1309, 1.0332,  ..., 1.2158, 1.1836, 1.1904]),\n",
       " 'model.layers.11.self_attn.k_proj': tensor([1.3721, 1.1309, 1.0332,  ..., 1.2158, 1.1836, 1.1904]),\n",
       " 'model.layers.11.self_attn.v_proj': tensor([1.3721, 1.1309, 1.0332,  ..., 1.2158, 1.1836, 1.1904]),\n",
       " 'model.layers.11.self_attn.o_proj': tensor([0.8340, 1.0303, 0.8882,  ..., 0.9946, 1.2734, 1.0146]),\n",
       " 'model.layers.11.mlp.gate_proj': tensor([1.0420, 0.8848, 0.7705,  ..., 0.8608, 0.8701, 0.8394]),\n",
       " 'model.layers.11.mlp.up_proj': tensor([1.0420, 0.8848, 0.7705,  ..., 0.8608, 0.8701, 0.8394]),\n",
       " 'model.layers.11.mlp.down_proj': tensor([1.3379, 1.6494, 0.6978,  ..., 1.4238, 1.2627, 1.1182]),\n",
       " 'model.layers.12.self_attn.q_proj': tensor([1.4639, 1.4736, 1.2783,  ..., 1.4316, 1.5059, 1.6689]),\n",
       " 'model.layers.12.self_attn.k_proj': tensor([1.4639, 1.4736, 1.2783,  ..., 1.4316, 1.5059, 1.6689]),\n",
       " 'model.layers.12.self_attn.v_proj': tensor([1.4639, 1.4736, 1.2783,  ..., 1.4316, 1.5059, 1.6689]),\n",
       " 'model.layers.12.self_attn.o_proj': tensor([0.9443, 0.8882, 1.0645,  ..., 1.4150, 1.1768, 1.1631]),\n",
       " 'model.layers.12.mlp.gate_proj': tensor([0.9448, 1.0420, 0.8398,  ..., 0.8882, 0.9424, 1.1191]),\n",
       " 'model.layers.12.mlp.up_proj': tensor([0.9448, 1.0420, 0.8398,  ..., 0.8882, 0.9424, 1.1191]),\n",
       " 'model.layers.12.mlp.down_proj': tensor([1.4854, 0.8633, 3.5723,  ..., 2.8906, 1.6230, 0.8784]),\n",
       " 'model.layers.13.self_attn.q_proj': tensor([1.3896, 1.6973, 1.3506,  ..., 1.5615, 1.3584, 1.8994]),\n",
       " 'model.layers.13.self_attn.k_proj': tensor([1.3896, 1.6973, 1.3506,  ..., 1.5615, 1.3584, 1.8994]),\n",
       " 'model.layers.13.self_attn.v_proj': tensor([1.3896, 1.6973, 1.3506,  ..., 1.5615, 1.3584, 1.8994]),\n",
       " 'model.layers.13.self_attn.o_proj': tensor([1.4854, 1.8076, 1.3115,  ..., 1.0371, 1.0547, 0.9585]),\n",
       " 'model.layers.13.mlp.gate_proj': tensor([0.9395, 1.1484, 0.8633,  ..., 0.9141, 0.9097, 1.1855]),\n",
       " 'model.layers.13.mlp.up_proj': tensor([0.9395, 1.1484, 0.8633,  ..., 0.9141, 0.9097, 1.1855]),\n",
       " 'model.layers.13.mlp.down_proj': tensor([1.0791, 1.2119, 2.6875,  ..., 1.0732, 3.7988, 1.1211]),\n",
       " 'model.layers.14.self_attn.q_proj': tensor([1.5713, 1.4326, 1.4648,  ..., 1.4746, 1.2852, 1.7539]),\n",
       " 'model.layers.14.self_attn.k_proj': tensor([1.5713, 1.4326, 1.4648,  ..., 1.4746, 1.2852, 1.7539]),\n",
       " 'model.layers.14.self_attn.v_proj': tensor([1.5713, 1.4326, 1.4648,  ..., 1.4746, 1.2852, 1.7539]),\n",
       " 'model.layers.14.self_attn.o_proj': tensor([1.0576, 2.6895, 1.2871,  ..., 1.7305, 1.5811, 1.4629]),\n",
       " 'model.layers.14.mlp.gate_proj': tensor([1.1631, 1.0410, 1.0420,  ..., 0.9302, 0.9351, 1.1201]),\n",
       " 'model.layers.14.mlp.up_proj': tensor([1.1631, 1.0410, 1.0420,  ..., 0.9302, 0.9351, 1.1201]),\n",
       " 'model.layers.14.mlp.down_proj': tensor([1.1152, 4.5039, 1.7656,  ..., 1.3350, 1.9277, 2.9121]),\n",
       " 'model.layers.15.self_attn.q_proj': tensor([1.4502, 1.3770, 1.3887,  ..., 1.4062, 1.2773, 1.5430]),\n",
       " 'model.layers.15.self_attn.k_proj': tensor([1.4502, 1.3770, 1.3887,  ..., 1.4062, 1.2773, 1.5430]),\n",
       " 'model.layers.15.self_attn.v_proj': tensor([1.4502, 1.3770, 1.3887,  ..., 1.4062, 1.2773, 1.5430]),\n",
       " 'model.layers.15.self_attn.o_proj': tensor([1.4287, 1.3740, 1.2305,  ..., 1.7217, 0.9644, 1.3945]),\n",
       " 'model.layers.15.mlp.gate_proj': tensor([1.1523, 1.0342, 1.0928,  ..., 0.9722, 0.9131, 1.0869]),\n",
       " 'model.layers.15.mlp.up_proj': tensor([1.1523, 1.0342, 1.0928,  ..., 0.9722, 0.9131, 1.0869]),\n",
       " 'model.layers.15.mlp.down_proj': tensor([3.8809, 2.1523, 8.8359,  ..., 1.4014, 0.8472, 1.6924]),\n",
       " 'model.layers.16.self_attn.q_proj': tensor([1.5488, 1.3193, 1.4014,  ..., 1.5225, 1.3477, 1.7188]),\n",
       " 'model.layers.16.self_attn.k_proj': tensor([1.5488, 1.3193, 1.4014,  ..., 1.5225, 1.3477, 1.7188]),\n",
       " 'model.layers.16.self_attn.v_proj': tensor([1.5488, 1.3193, 1.4014,  ..., 1.5225, 1.3477, 1.7188]),\n",
       " 'model.layers.16.self_attn.o_proj': tensor([2.1113, 1.7461, 1.4268,  ..., 1.4951, 2.1641, 1.2969]),\n",
       " 'model.layers.16.mlp.gate_proj': tensor([1.1309, 1.0742, 1.2666,  ..., 1.1387, 1.0273, 1.2354]),\n",
       " 'model.layers.16.mlp.up_proj': tensor([1.1309, 1.0742, 1.2666,  ..., 1.1387, 1.0273, 1.2354]),\n",
       " 'model.layers.16.mlp.down_proj': tensor([2.0254, 6.6602, 1.9951,  ..., 4.1172, 1.8623, 0.6987]),\n",
       " 'model.layers.17.self_attn.q_proj': tensor([1.6406, 1.4277, 1.4268,  ..., 1.7051, 1.5010, 1.7109]),\n",
       " 'model.layers.17.self_attn.k_proj': tensor([1.6406, 1.4277, 1.4268,  ..., 1.7051, 1.5010, 1.7109]),\n",
       " 'model.layers.17.self_attn.v_proj': tensor([1.6406, 1.4277, 1.4268,  ..., 1.7051, 1.5010, 1.7109]),\n",
       " 'model.layers.17.self_attn.o_proj': tensor([1.1348, 1.3359, 1.2676,  ..., 1.7852, 1.4023, 1.7178]),\n",
       " 'model.layers.17.mlp.gate_proj': tensor([1.2637, 1.2051, 1.1533,  ..., 1.3252, 1.2559, 1.2305]),\n",
       " 'model.layers.17.mlp.up_proj': tensor([1.2637, 1.2051, 1.1533,  ..., 1.3252, 1.2559, 1.2305]),\n",
       " 'model.layers.17.mlp.down_proj': tensor([5.1719, 3.2383, 2.1875,  ..., 4.2578, 4.5234, 2.1445]),\n",
       " 'model.layers.18.self_attn.q_proj': tensor([1.6787, 1.4873, 1.4941,  ..., 1.9277, 1.6719, 1.6914]),\n",
       " 'model.layers.18.self_attn.k_proj': tensor([1.6787, 1.4873, 1.4941,  ..., 1.9277, 1.6719, 1.6914]),\n",
       " 'model.layers.18.self_attn.v_proj': tensor([1.6787, 1.4873, 1.4941,  ..., 1.9277, 1.6719, 1.6914]),\n",
       " 'model.layers.18.self_attn.o_proj': tensor([2.9258, 2.2617, 2.4004,  ..., 1.0947, 1.2822, 1.2930]),\n",
       " 'model.layers.18.mlp.gate_proj': tensor([1.3672, 1.2266, 1.2725,  ..., 1.4629, 1.2773, 1.2031]),\n",
       " 'model.layers.18.mlp.up_proj': tensor([1.3672, 1.2266, 1.2725,  ..., 1.4629, 1.2773, 1.2031]),\n",
       " 'model.layers.18.mlp.down_proj': tensor([5.6797, 3.0879, 2.6387,  ..., 6.1328, 1.5537, 2.5879]),\n",
       " 'model.layers.19.self_attn.q_proj': tensor([1.5215, 1.4014, 1.6934,  ..., 1.7598, 1.5107, 1.6904]),\n",
       " 'model.layers.19.self_attn.k_proj': tensor([1.5215, 1.4014, 1.6934,  ..., 1.7598, 1.5107, 1.6904]),\n",
       " 'model.layers.19.self_attn.v_proj': tensor([1.5215, 1.4014, 1.6934,  ..., 1.7598, 1.5107, 1.6904]),\n",
       " 'model.layers.19.self_attn.o_proj': tensor([1.7705, 2.1582, 1.8584,  ..., 1.8252, 2.4570, 2.3574]),\n",
       " 'model.layers.19.mlp.gate_proj': tensor([1.3818, 1.2012, 1.4512,  ..., 1.3311, 1.2305, 1.3242]),\n",
       " 'model.layers.19.mlp.up_proj': tensor([1.3818, 1.2012, 1.4512,  ..., 1.3311, 1.2305, 1.3242]),\n",
       " 'model.layers.19.mlp.down_proj': tensor([3.1855, 4.2656, 6.1719,  ..., 1.9521, 8.2188, 4.7305]),\n",
       " 'model.layers.20.self_attn.q_proj': tensor([1.7051, 1.7373, 1.8037,  ..., 1.6680, 1.6260, 1.7139]),\n",
       " 'model.layers.20.self_attn.k_proj': tensor([1.7051, 1.7373, 1.8037,  ..., 1.6680, 1.6260, 1.7139]),\n",
       " 'model.layers.20.self_attn.v_proj': tensor([1.7051, 1.7373, 1.8037,  ..., 1.6680, 1.6260, 1.7139]),\n",
       " 'model.layers.20.self_attn.o_proj': tensor([2.0430, 2.5273, 2.4258,  ..., 1.9219, 1.9912, 1.6709]),\n",
       " 'model.layers.20.mlp.gate_proj': tensor([1.4639, 1.4131, 1.5449,  ..., 1.3428, 1.3936, 1.3252]),\n",
       " 'model.layers.20.mlp.up_proj': tensor([1.4639, 1.4131, 1.5449,  ..., 1.3428, 1.3936, 1.3252]),\n",
       " 'model.layers.20.mlp.down_proj': tensor([2.9531, 3.5176, 2.9434,  ..., 1.4521, 4.0898, 4.0352]),\n",
       " 'model.layers.21.self_attn.q_proj': tensor([1.7783, 1.6621, 1.8496,  ..., 1.6621, 1.6084, 1.7979]),\n",
       " 'model.layers.21.self_attn.k_proj': tensor([1.7783, 1.6621, 1.8496,  ..., 1.6621, 1.6084, 1.7979]),\n",
       " 'model.layers.21.self_attn.v_proj': tensor([1.7783, 1.6621, 1.8496,  ..., 1.6621, 1.6084, 1.7979]),\n",
       " 'model.layers.21.self_attn.o_proj': tensor([1.8838, 1.6846, 1.5781,  ..., 1.8066, 2.2305, 2.2207]),\n",
       " 'model.layers.21.mlp.gate_proj': tensor([1.4648, 1.5322, 1.5967,  ..., 1.3945, 1.3975, 1.5059]),\n",
       " 'model.layers.21.mlp.up_proj': tensor([1.4648, 1.5322, 1.5967,  ..., 1.3945, 1.3975, 1.5059]),\n",
       " 'model.layers.21.mlp.down_proj': tensor([2.4434, 2.8730, 3.5566,  ..., 2.5977, 4.2383, 3.1680]),\n",
       " 'model.layers.22.self_attn.q_proj': tensor([1.5273, 1.6787, 1.9746,  ..., 1.7705, 1.7109, 1.9492]),\n",
       " 'model.layers.22.self_attn.k_proj': tensor([1.5273, 1.6787, 1.9746,  ..., 1.7705, 1.7109, 1.9492]),\n",
       " 'model.layers.22.self_attn.v_proj': tensor([1.5273, 1.6787, 1.9746,  ..., 1.7705, 1.7109, 1.9492]),\n",
       " 'model.layers.22.self_attn.o_proj': tensor([2.4609, 1.5117, 1.5664,  ..., 1.3877, 1.1289, 1.5615]),\n",
       " 'model.layers.22.mlp.gate_proj': tensor([1.3984, 1.4482, 1.6982,  ..., 1.5361, 1.4746, 1.6172]),\n",
       " 'model.layers.22.mlp.up_proj': tensor([1.3984, 1.4482, 1.6982,  ..., 1.5361, 1.4746, 1.6172]),\n",
       " 'model.layers.22.mlp.down_proj': tensor([8.7734, 5.9258, 3.6699,  ..., 1.7900, 1.7764, 4.5898]),\n",
       " 'model.layers.23.self_attn.q_proj': tensor([1.5908, 1.7666, 1.8252,  ..., 1.8945, 1.6172, 1.7910]),\n",
       " 'model.layers.23.self_attn.k_proj': tensor([1.5908, 1.7666, 1.8252,  ..., 1.8945, 1.6172, 1.7910]),\n",
       " 'model.layers.23.self_attn.v_proj': tensor([1.5908, 1.7666, 1.8252,  ..., 1.8945, 1.6172, 1.7910]),\n",
       " 'model.layers.23.self_attn.o_proj': tensor([1.9854, 1.4219, 1.5840,  ..., 2.7676, 1.8525, 1.9922]),\n",
       " 'model.layers.23.mlp.gate_proj': tensor([1.4766, 1.6543, 1.6338,  ..., 1.5547, 1.4980, 1.5283]),\n",
       " 'model.layers.23.mlp.up_proj': tensor([1.4766, 1.6543, 1.6338,  ..., 1.5547, 1.4980, 1.5283]),\n",
       " 'model.layers.23.mlp.down_proj': tensor([3.7188, 7.4219, 4.0391,  ..., 2.6875, 3.8809, 2.7559]),\n",
       " 'model.layers.24.self_attn.q_proj': tensor([1.7402, 1.9512, 1.9150,  ..., 1.8086, 1.7676, 2.0703]),\n",
       " 'model.layers.24.self_attn.k_proj': tensor([1.7402, 1.9512, 1.9150,  ..., 1.8086, 1.7676, 2.0703]),\n",
       " 'model.layers.24.self_attn.v_proj': tensor([1.7402, 1.9512, 1.9150,  ..., 1.8086, 1.7676, 2.0703]),\n",
       " 'model.layers.24.self_attn.o_proj': tensor([2.2793, 2.2480, 2.1152,  ..., 2.3750, 2.6797, 2.6055]),\n",
       " 'model.layers.24.mlp.gate_proj': tensor([1.6309, 1.6279, 1.7129,  ..., 1.5625, 1.5195, 1.7764]),\n",
       " 'model.layers.24.mlp.up_proj': tensor([1.6309, 1.6279, 1.7129,  ..., 1.5625, 1.5195, 1.7764]),\n",
       " 'model.layers.24.mlp.down_proj': tensor([28.3750,  2.4258,  5.9375,  ...,  2.7637,  9.8516,  3.5938]),\n",
       " 'model.layers.25.self_attn.q_proj': tensor([2.0586, 1.9609, 2.0508,  ..., 1.8389, 1.7402, 2.0312]),\n",
       " 'model.layers.25.self_attn.k_proj': tensor([2.0586, 1.9609, 2.0508,  ..., 1.8389, 1.7402, 2.0312]),\n",
       " 'model.layers.25.self_attn.v_proj': tensor([2.0586, 1.9609, 2.0508,  ..., 1.8389, 1.7402, 2.0312]),\n",
       " 'model.layers.25.self_attn.o_proj': tensor([1.3086, 1.2373, 1.2725,  ..., 2.4688, 2.5840, 1.7158]),\n",
       " 'model.layers.25.mlp.gate_proj': tensor([1.7988, 1.7100, 1.7793,  ..., 1.5244, 1.5449, 1.7246]),\n",
       " 'model.layers.25.mlp.up_proj': tensor([1.7988, 1.7100, 1.7793,  ..., 1.5244, 1.5449, 1.7246]),\n",
       " 'model.layers.25.mlp.down_proj': tensor([ 3.9727, 24.3125,  4.9180,  ...,  2.5508,  3.6699,  3.8789]),\n",
       " 'model.layers.26.self_attn.q_proj': tensor([2.1523, 2.1484, 2.2402,  ..., 2.0840, 1.9287, 2.0801]),\n",
       " 'model.layers.26.self_attn.k_proj': tensor([2.1523, 2.1484, 2.2402,  ..., 2.0840, 1.9287, 2.0801]),\n",
       " 'model.layers.26.self_attn.v_proj': tensor([2.1523, 2.1484, 2.2402,  ..., 2.0840, 1.9287, 2.0801]),\n",
       " 'model.layers.26.self_attn.o_proj': tensor([3.4082, 2.6328, 2.2188,  ..., 2.0176, 1.8330, 2.6055]),\n",
       " 'model.layers.26.mlp.gate_proj': tensor([1.7549, 1.7559, 1.8457,  ..., 1.6572, 1.6230, 1.7637]),\n",
       " 'model.layers.26.mlp.up_proj': tensor([1.7549, 1.7559, 1.8457,  ..., 1.6572, 1.6230, 1.7637]),\n",
       " 'model.layers.26.mlp.down_proj': tensor([3.9863, 2.9980, 3.2891,  ..., 2.3516, 3.1895, 7.7305]),\n",
       " 'model.layers.27.self_attn.q_proj': tensor([2.1387, 2.0469, 2.3535,  ..., 2.0449, 2.1543, 2.1309]),\n",
       " 'model.layers.27.self_attn.k_proj': tensor([2.1387, 2.0469, 2.3535,  ..., 2.0449, 2.1543, 2.1309]),\n",
       " 'model.layers.27.self_attn.v_proj': tensor([2.1387, 2.0469, 2.3535,  ..., 2.0449, 2.1543, 2.1309]),\n",
       " 'model.layers.27.self_attn.o_proj': tensor([2.0059, 1.7100, 1.5898,  ..., 1.6572, 1.7520, 2.0254]),\n",
       " 'model.layers.27.mlp.gate_proj': tensor([2.0645, 1.6123, 1.9473,  ..., 1.7266, 1.7754, 1.8145]),\n",
       " 'model.layers.27.mlp.up_proj': tensor([2.0645, 1.6123, 1.9473,  ..., 1.7266, 1.7754, 1.8145]),\n",
       " 'model.layers.27.mlp.down_proj': tensor([5.2305, 2.7891, 6.3516,  ..., 5.3008, 2.1602, 3.0938]),\n",
       " 'model.layers.28.self_attn.q_proj': tensor([2.2012, 1.8984, 2.1582,  ..., 1.9131, 2.0645, 2.1250]),\n",
       " 'model.layers.28.self_attn.k_proj': tensor([2.2012, 1.8984, 2.1582,  ..., 1.9131, 2.0645, 2.1250]),\n",
       " 'model.layers.28.self_attn.v_proj': tensor([2.2012, 1.8984, 2.1582,  ..., 1.9131, 2.0645, 2.1250]),\n",
       " 'model.layers.28.self_attn.o_proj': tensor([1.9922, 2.6523, 2.6855,  ..., 2.1230, 1.8848, 1.9922]),\n",
       " 'model.layers.28.mlp.gate_proj': tensor([2.0996, 1.6963, 2.0781,  ..., 1.7617, 1.9473, 1.8271]),\n",
       " 'model.layers.28.mlp.up_proj': tensor([2.0996, 1.6963, 2.0781,  ..., 1.7617, 1.9473, 1.8271]),\n",
       " 'model.layers.28.mlp.down_proj': tensor([4.3945, 2.0020, 3.2480,  ..., 4.5195, 3.9844, 2.8066]),\n",
       " 'model.layers.29.self_attn.q_proj': tensor([2.1465, 2.1211, 2.0488,  ..., 2.3242, 1.9834, 2.0059]),\n",
       " 'model.layers.29.self_attn.k_proj': tensor([2.1465, 2.1211, 2.0488,  ..., 2.3242, 1.9834, 2.0059]),\n",
       " 'model.layers.29.self_attn.v_proj': tensor([2.1465, 2.1211, 2.0488,  ..., 2.3242, 1.9834, 2.0059]),\n",
       " 'model.layers.29.self_attn.o_proj': tensor([2.8984, 2.7695, 2.4922,  ..., 3.1133, 3.1191, 2.8672]),\n",
       " 'model.layers.29.mlp.gate_proj': tensor([2.0840, 1.8486, 1.7744,  ..., 1.9453, 1.7822, 1.6504]),\n",
       " 'model.layers.29.mlp.up_proj': tensor([2.0840, 1.8486, 1.7744,  ..., 1.9453, 1.7822, 1.6504]),\n",
       " 'model.layers.29.mlp.down_proj': tensor([3.4707, 6.8359, 2.9629,  ..., 7.3945, 2.7793, 3.8867]),\n",
       " 'model.layers.30.self_attn.q_proj': tensor([1.9863, 2.2031, 1.8574,  ..., 2.1055, 1.9434, 1.8389]),\n",
       " 'model.layers.30.self_attn.k_proj': tensor([1.9863, 2.2031, 1.8574,  ..., 2.1055, 1.9434, 1.8389]),\n",
       " 'model.layers.30.self_attn.v_proj': tensor([1.9863, 2.2031, 1.8574,  ..., 2.1055, 1.9434, 1.8389]),\n",
       " 'model.layers.30.self_attn.o_proj': tensor([1.7891, 1.9951, 2.0820,  ..., 1.9443, 2.8906, 2.3203]),\n",
       " 'model.layers.30.mlp.gate_proj': tensor([1.9492, 2.0137, 1.6748,  ..., 1.6514, 1.7617, 1.6152]),\n",
       " 'model.layers.30.mlp.up_proj': tensor([1.9492, 2.0137, 1.6748,  ..., 1.6514, 1.7617, 1.6152]),\n",
       " 'model.layers.30.mlp.down_proj': tensor([4.7383, 2.3477, 3.5938,  ..., 3.8535, 3.2344, 4.3242]),\n",
       " 'model.layers.31.self_attn.q_proj': tensor([1.3008, 1.7773, 1.6025,  ..., 1.5869, 1.7871, 1.6074]),\n",
       " 'model.layers.31.self_attn.k_proj': tensor([1.3008, 1.7773, 1.6025,  ..., 1.5869, 1.7871, 1.6074]),\n",
       " 'model.layers.31.self_attn.v_proj': tensor([1.3008, 1.7773, 1.6025,  ..., 1.5869, 1.7871, 1.6074]),\n",
       " 'model.layers.31.self_attn.o_proj': tensor([2.4277, 2.4375, 1.7812,  ..., 2.1875, 2.0293, 1.9873]),\n",
       " 'model.layers.31.mlp.gate_proj': tensor([1.7646, 1.7412, 1.6123,  ..., 1.5000, 1.8301, 1.4580]),\n",
       " 'model.layers.31.mlp.up_proj': tensor([1.7646, 1.7412, 1.6123,  ..., 1.5000, 1.8301, 1.4580]),\n",
       " 'model.layers.31.mlp.down_proj': tensor([3.2305, 3.8262, 9.5547,  ..., 4.0859, 2.5820, 3.1660]),\n",
       " 'lm_head': tensor([9.9922, 5.6094, 5.8555,  ..., 7.1133, 6.5781, 6.1445])}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968944099378882"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_lm(model, act_scales, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968944099378882"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-7b-smooth-0.5/tokenizer_config.json',\n",
       " 'llama-7b-smooth-0.5/special_tokens_map.json',\n",
       " 'llama-7b-smooth-0.5/tokenizer.model',\n",
       " 'llama-7b-smooth-0.5/added_tokens.json')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('llama-7b-smooth-0.5')  \n",
    "tokenizer.save_pretrained('llama-7b-smooth-0.5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t  pytorch_model.bin.index.json\r\n",
      "generation_config.json\t\t  special_tokens_map.json\r\n",
      "pytorch_model-00001-of-00002.bin  tokenizer_config.json\r\n",
      "pytorch_model-00002-of-00002.bin  tokenizer.model\r\n"
     ]
    }
   ],
   "source": [
    "! ls llama-7b-smooth-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t  pytorch_model.bin.index.json\r\n",
      "generation_config.json\t\t  special_tokens_map.json\r\n",
      "pytorch_model-00001-of-00002.bin  tokenizer_config.json\r\n",
      "pytorch_model-00002-of-00002.bin  tokenizer.model\r\n"
     ]
    }
   ],
   "source": [
    "! ls /home/alekseev_v/models/smooth-0.5/llama-7b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"architectures\": [\"LLaMAForCausalLM\"], \"bos_token_id\": 0, \"eos_token_id\": 1, \"hidden_act\": \"silu\", \"hidden_size\": 4096, \"intermediate_size\": 11008, \"initializer_range\": 0.02, \"max_sequence_length\": 2048, \"model_type\": \"llama\", \"num_attention_heads\": 32, \"num_hidden_layers\": 32, \"pad_token_id\": -1, \"rms_norm_eps\": 1e-06, \"torch_dtype\": \"float16\", \"transformers_version\": \"4.27.0.dev0\", \"use_cache\": true, \"vocab_size\": 32000}"
     ]
    }
   ],
   "source": [
    "! cat /home/alekseev_v/models/llama-7b/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"_name_or_path\": \"/home/alekseev_v/models/llama-7b\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"LlamaForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 4096,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 11008,\r\n",
      "  \"max_position_embeddings\": 2048,\r\n",
      "  \"max_sequence_length\": 2048,\r\n",
      "  \"model_type\": \"llama\",\r\n",
      "  \"num_attention_heads\": 32,\r\n",
      "  \"num_hidden_layers\": 32,\r\n",
      "  \"num_key_value_heads\": 32,\r\n",
      "  \"pad_token_id\": -1,\r\n",
      "  \"pretraining_tp\": 1,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_scaling\": null,\r\n",
      "  \"rope_theta\": 10000.0,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"torch_dtype\": \"float16\",\r\n",
      "  \"transformers_version\": \"4.33.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32000\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! cat /home/alekseev_v/models/smooth-0.5/llama-7b/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (1): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (2): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (3): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (27): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (29): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (30): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (31): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_smoothquant_w8a8 = quantize_model(model)\n",
    "print(model_smoothquant_w8a8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the smoothed model has the same accuracy as the FP16 model. This is because SmoothQuant smooths the outliers in activations and moves the quantization difficulty from activations to weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmoothQuant W8A8 quantized model accuracy: 0.14596273291925466\n"
     ]
    }
   ],
   "source": [
    "print(f'SmoothQuant W8A8 quantized model accuracy: {acc_smoothquant_w8a8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoothquant",
   "language": "python",
   "name": "smoothquant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
